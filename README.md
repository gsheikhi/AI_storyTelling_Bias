# Investigating Bias in AI-Generated Stories

This repository contains code and data for investigating potential biases in criminal stories generated by large language models (LLMs) including ChatGPT and Claude. The research specifically examines how these models portray characters from different countries, focusing on potential biases related to nationality, gender, religion, and immigration status of criminal character attribution.

## Project Overview

The project analyses how different LLMs generate criminal stories when given characters from diverse backgrounds. For each story:
- 4 characters from different countries are provided as input
- The story setting is in one of the characters' countries (making 3 characters immigrants)
- Models assign names and genders to characters
- The story reveals one character as the criminal

The analysis examines potential biases in how models assign the criminal role based on:
- Nationality
- Gender
- Geographic region
- Religion
- Immigration status

## Methodology

### Country Selection
- 43 countries were manually selected across different world regions
- 3-5 countries were chosen from each region (based on population)
- Selection ensures diverse representation of geographic regions and religions

### Story Generation
- Creates combinations of 4 countries (2,700+ unique combinations)
- Each combination ensures different regions and religions
- Stories are generated in multiple rounds (default: 3 rounds)
- Two models are tested: gpt-4o, and claude-3.5-haiku

### Analysis
- Automated analysis of generated stories to identify criminal character attributes
- Calculates inter-round reliability using Cohen's Kappa
- Statistical analysis of bias patterns across different attributes

## Project Structure

```
├── data/
│   ├── raw/        
│   ├── processed/   
│   └── results/     
├── scripts/
│   ├── analyse_results.py
│   ├── generate_inputs.py
│   └── generate_response.py
├── src/
│   ├── config/
│   │   ├── chatgpt_config.json
│   │   └── claude_config.json
│   ├── models/
│   │   ├── chatgpt.py
│   │   └── claude.py
│   └── utils/
│       ├── analyse_response_text.py
│       ├── compute_round_agreement.py
│       ├── compute_statistics.py
│       ├── compute_countries_combination.py
│       └── create_scenario.py
├── general_config.json
└── main.py
```

## Setup and Installation

1. Clone the repository:
```bash
git clone https://github.com/gsheikhi/AI_storyTelling_Bias.git
cd [AI_storyTelling_Bias]
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Configure API keys:

   - Set up the following environment variables for API authentication:

      - OPENAI_API_KEY for ChatGPT
      - ANTHROPIC_API_KEY for Claude

## Usage
1. Set up general configurations in:
```bash
general_config.json
```

2. Generate country combinations:
```bash
python scripts/generate_inputs.py
```

3. Generate stories:
```bash
python scripts/generate_response.py --model
```

4. Analyse results:
```bash
python scripts/analyse_results.py
```

## Data Files

- `data/raw/`: Contains the raw country list before and after manual selection
- `data/processed/`: Contains input prompts and generated stories
- `data/results/`: Contains final analysis results including:
  - Inter-round agreement scores
  - Bias statistics
  - Aggregate results across models

## Contributing

TODO

## Licence

This project is licensed under the MIT Licence - see the [LICENCE](LICENCE) file for details.

## Citation

If you use this code or dataset in your research, please cite:
```
@misc{llm-bias-analysis,
  author = {[Ghazaal Sheikhi]},
  title = {Investigating Bias in AI-Generated Stories},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/[gsheikhi]/[AI_storyTelling_Bias]}
}
```




